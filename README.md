

# Spark développer pour le Big Data
<img src="https://github.com/rbizoi/Spark-developper-pour-le-Big-Data/blob/master/images/presentation.png" width="512">



## 01-L'écosystème Big Data Analytique

Une architecture de type Big Data Analytique doit permettre d'ingérer, consolider, traiter et analyser à grande vitesse en flux continu les données. Le traitement de flux doit être rapide, évolutif, tolérant aux pannes et de bout en bout, sans que l'utilisateur ait à se soucier du flux.<br>
<img src="https://github.com/rbizoi/Spark-developper-pour-le-Big-Data/blob/master/Chapitre-01/images/M01.06.png" width="512">

La structure de la machine virtuelle.<br>

<img src="https://github.com/rbizoi/Spark-developper-pour-le-Big-Data/blob/master/Chapitre-01/images/M01.07.png" width="512">

1.	      Le déluge de données	1
2.	      Les systèmes de calcul distribué	2
3.	      Le système de fichiers distribué Hadoop (HDFS)	2
4.	      Le système de traitements MapReduce	4
5.	      Les extensions MapReduce	5
6.	      Le Big Data Analytique	6
6.1.      	La découverte	6
6.2.      	La préparation des données	6
6.3.      	La conception du modèle	7
6.4.      	La construction du modèle	7
6.5.      	Les résultats	7
6.6.      	La mise en production	8
7.	      L’analyse de données en flux continu	8
8.	      L’installation	9
9.	      Les produits nécessaires	9
10.	      Installer les prérequis	10
10.1.   	La configuration du système d'exploitation	10
10.2.   	L'installation de l'environnement Java	11
10.3.   	L'installation du langage Python	12
10.4.   	L'installation du langage R	12
10.5.   	La création des utilisateurs	13
10.6.   	La configuration automatique des prérequis	14
11.	Installer Apache Hadoop	15
12.	Installer Apache Spark	15
12.1.	L'installation des fichiers	15
12.2.	La configuration de l'environnement	17
12.3.	Les environnements de commandes	18
12.3.1.	L'environnement de commande Scala	18
12.3.2.	L'environnement de commande Python	19
12.3.3.	L'environnement de commande R	20
12.4.	L'installation et intégration avec Apache Hadoop	20
13.	Installer Apache Hive	24
14.	Installer Apache Zookeeper	25
15.	Installer Apache Kafka	26
16.	Installer Apache Zeppelin	26
17.	Configurer le démarrage et l'arrêt du cluster	26
17.1.	Apache Hadoop	27
17.2.	Jupyter Notebook	29
17.3.	Apache Zeppelin	31




## 02-L’architecture

## 03-La structure et les types de données

## 04-Les traitements et le flux de données

## 05-L’exploration, la préparation et la visualisation des données

## 06-Le « Machine Learning »

## 07-Le « Deep Learning »

## 08-La mise en production
